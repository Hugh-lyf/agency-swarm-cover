import requests
import sqlite3
import re
from time import sleep
from requests.exceptions import RequestException
from bs4 import BeautifulSoup, Tag
from typing import List

DOWNLOAD_INTERVAL = 0.1 # second

def init_database_for_download(db_file: str):
    '''
    Create tables with defined data schema
    '''
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()

    cursor.execute('DROP TABLE IF EXISTS pages;')
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS pages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT NOT NULL UNIQUE,
            html TEXT
        )''')

    conn.commit()
    conn.close()
    return

def delete_pages(db_file: str):
    conn = sqlite3.connect(db_file)
    cursor = conn.cursor()
    cursor.execute('DROP TABLE IF EXISTS pages;')
    cursor.execute('VACUUM;')
    conn.commit()
    conn.close()

def robust_get(url: str, params=None, headers=None, max_retries=5, sleep_unit=1) -> requests.Response:
    """
    A wrapper for requests.get() that retries the request with exponential backoff in case of errors.

    Args:
        url (str): The URL for the GET request.
        params (dict, optional): Query parameters to send with the request. Defaults to None.
        headers (dict, optional): Headers to include in the request. Defaults to None.
        max_retries (int): Maximum number of retries in case of failure. Defaults to 5.
        sleep_unit (int or float): Factor to determine how long to sleep between retries.
                                   Defaults to 1 second (1, 2, 4, 8...).

    Returns:
        requests.Response: The response object from the GET request.

    Raises:
        RequestException: If the request fails after the maximum number of retries.
    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, params=params, headers=headers)
            response.raise_for_status()
            return response
        except Exception as e:
            retries += 1
            sleep_time = sleep_unit * (2 ** (retries - 1)) # Exponential backoff
            print(f"robust_get({url}): attempt {retries} failed: {e}. Retrying in {sleep_time:.1f} seconds...")
            sleep(sleep_time)

    raise RequestException(f"Failed to get a successful response after {max_retries} attempts.")

def download_and_save_to_html(url: str, file_name: str = None):
    '''
    Download url and save to a .html file
    '''
    try:
        # 1. download the webpage
        response = robust_get(url)
        html_content = response.text

        # 2. save to file
        if file_name is None:
            file_name = url.split('/')[-1]
        if not file_name.endswith(".html"):
            file_name = file_name + ".html"
        with open(file_name, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"page successfully saved to {file_name}.")
    except RequestException as e:
        print(f"failed to download the webpage: {e}")

def download_and_save_to_db(url: str, db_file: str | None = None, cursor: sqlite3.Cursor | None = None):
    '''
    Download url and save to sqlite database
    '''
    if cursor is None:
        conn = sqlite3.connect(db_file)
        cursor = conn.cursor()
        created_cursor = True
    else:
        created_cursor = False
    try:
        response = robust_get(url)
        html_content = response.text

        # doc errors
        # this row referenced the wrong table 
        if url == "https://support.huaweicloud.com/api-ecs/ecs_02_1901.html":
            html_content = html_content.replace('规格的单vCPU对应内存容量范围，不填表示接受所有，内存单位GiB，详情请参见<a href="#ecs_02_1901__table139181731183815">表6</a>', '规格的单vCPU对应内存容量范围，不填表示接受所有，内存单位GiB，详情请参见<a href="#ecs_02_1901__table13679155894717">表7</a>')
        # id 'section8' is generated by javascript, therefore does not exist in downloaded HTML
        html_content = html_content.replace('href="https://support.huaweicloud.com/api-ecs/zh-cn_topic_0167957246.html#section8"', 'href="https://support.huaweicloud.com/api-ecs/zh-cn_topic_0167957246.html#ZH-CN_TOPIC_0167957246__section1361484104817"')
        # this row used the wrong type
        if url == "https://support.huaweicloud.com/api-cce/cce_02_0356.html":
            html_content = re.sub(r"nodeNameTemplate.*?Object", lambda match: match.group(0).replace("Object", "Map<String,String>"), html_content, flags=re.DOTALL)
        if url == "https://support.huaweicloud.com/api-cce/cce_02_0242.html":
            html_content = re.sub(r"nodepoolScaleUp.*?否", lambda match: match.group(0).replace("否", "是"), html_content, flags=re.DOTALL)
            html_content = re.sub(r"批量创建时节点的个数", lambda match: match.group(0).replace("批量创建时节点的个数", "一个或多个创建节点时的节点个数"), html_content, flags=re.DOTALL)
        
        cursor.execute("""
            INSERT INTO pages (url, html)
            VALUES (?, ?)
        """, (url, html_content))

        if created_cursor:
            conn.commit()
            conn.close()
        print(f"SUCCESS\t{url} downloaded and saved to {db_file if db_file is not None else 'cursor'}")

    except Exception as e:
        print(f"ERROR\t{url} is not saved because an error occured: {e}")
        if created_cursor:
            conn.rollback()
            conn.close()

def get_doc_pages(leftmenu_url):
    """
    Generator that extracts and yields clickable URLs under the "API参考-API" 
    section in the given product's documentation.

    Args:
        leftmenu_url (str): The URL of the Huawei Cloud documentation leftmenu page.

    Yields:
        str: A clickable URL under "API参考-API" section.
    """
    def find_li_by_name(lis: List[Tag], name: str) -> Tag | None:
        for li in lis:
            li_name = li.find("a", recursive=False)
            if li_name is not None and li_name.get_text().strip() == name:
                return li
        return None

    # 1. download the page
    response = robust_get(leftmenu_url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # 2. go down the tree
    side_nav_ul = soup.find('ul', class_="side-nav")
    side_nav_lis = side_nav_ul.find_all('li', recursive=False)

    api_ref_li = find_li_by_name(side_nav_lis, "API参考")
    assert api_ref_li is not None, "Could not find API参考!"

    api_ref_ul = api_ref_li.ul
    assert api_ref_ul is not None, "API参考 has no ul!"
    api_ref_lis = api_ref_ul.find_all('li', recursive=False)

    api_li = find_li_by_name(api_ref_lis, "API")

    def extract_from_li(li):
        if li.ul is None:
            return
        # iterate through <li> elements within the section
        for son_li in li.ul.find_all('li', recursive=False):            
            # extract the URL
            if son_li.a:
                url = son_li.a.get('href')
                if url and url.startswith('https'):
                    yield url
            
            # recursively process nested <ul>
            yield from extract_from_li(son_li)

    yield from extract_from_li(api_li)

def download(db_file: str, services: List[str]):
    init_database_for_download(db_file)

    leftmenu_url = "https://support.huaweicloud.com/{service}/v3_support_leftmenu_fragment.html"

    for service in services:
        for doc_page in get_doc_pages(leftmenu_url.format(service=service)):
            download_and_save_to_db(doc_page, db_file)
            sleep(DOWNLOAD_INTERVAL)
            # input("press enter to continue...")

if __name__ == "__main__":
    download_and_save_to_html("https://support.huaweicloud.com/api-ecs/zh-cn_topic_0167957246.html")
    # download(db_file="download.sqlite", services = ['ecs'])
